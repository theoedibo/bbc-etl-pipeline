# Objective

This project is a modular ETL (Extract, Transform, Load) pipeline that automatically scrapes news headlines from the BBC News website, cleans and structures the data using pandas, and stores the results in a SQLite database and CSV file.

The pipeline is designed using production-style principles including modular architecture, logging, error handling, timestamping, and append-based data storage to maintain historical records.

The pipeline executes successfully and logs execution details for monitoring and debugging purposes.

---

# Objectives

* Automate web data extraction
* Implement structured ETL architecture
* Ensure data traceability using timestamps
* Enable historical data storage via append logic
* Introduce logging for monitoring and reliability

---

# Architecture Overview

```
Windows Task Scheduler
          â†“
        main.py
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Extract Layer â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Transform Layerâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Load Layer   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
 SQLite DB + CSV Output
```

---

# Technologies Used

* Python
* pandas
* BeautifulSoup (bs4)
* requests
* SQLite (sqlite3)
* Python logging module
* Windows Task Scheduler

---

# Pipeline Features

âœ” Modular structure (Extract, Transform, Load separated)
âœ” Centralized logging (`pipeline.log`)
âœ” Error handling across stages
âœ” Timestamp column (`scraped_at`)
âœ” Append-based database loading (historical tracking)
âœ” CSV export
âœ” Automated execution

---

# Project Structure

```
bbc-etl-pipeline/
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline.log
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ news.db
â”œâ”€â”€ bbc_news.csv
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# Database Design

Table: `news_titles`

| Column     | Description                 |
| ---------- | --------------------------- |
| S/N        | Serial number               |
| Title      | Headline text               |
| scraped_at | Timestamp of data ingestion |

The table uses append logic to preserve historical data across runs.

---

# What This Project Demonstrates

* Understanding of ETL architecture
* Modular software design
* Production-style logging
* Data lineage through timestamps
* Reliable repeated execution
* Clean project structuring

---

# Future Enhancements

* Add incremental loading (avoid duplicate ingestion)
* Migrate from SQLite to PostgreSQL
* Add unit testing
* Containerize using Docker
* Replace Windows Scheduler with workflow orchestration tool
* Deploy on cloud (AWS/GCP)




* I can help you write a LinkedIn project showcase post
* Or design your next advanced ETL (crypto or health API based)
* Or help you turn this into a CV bullet that passes ATS

Youâ€™re moving fast now ğŸ’ªğŸ”¥
